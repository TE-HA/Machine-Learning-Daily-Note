# 贝叶斯

​		贝叶斯决策论(Bayesian decision theory)是概率框架下实施决策的基本方法。对分类任务来说，在所有相关概率均已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

​		why贝叶斯？

1. 现实世界中很多问题是不确定的，人的观察能力有限

  		2. 我们日常观察的只是事物的表面结果，因此我们需要为其提供一个准则

**贝叶斯公式：**
$$
P(A|B)=\frac {P(A)·P(B|A)}{P(B)}
$$
其中P(A)是类**“先验概率”**，表达样本空间中各类样本所占的比例。根据大数定律，当训练集包含充足的独立同分布样本时，P（A）可通过各类样本出现的频率来进行估计。

## 例子1 拼写纠正

​		问题时我们看到用户输入了一个不在字典中的新词，我们需要将其预测为他本来想写的那个词。*the* --> *tha* e.tc.
$$
P(我们猜测他想输入的词|他实际输入的词)
$$
​		用户实际输入的词我们记为D（表示data，为观测数据），**猜测1：**P(h1|D)，**猜测2：**P（h2|D）。。。统称为**P（h|D）**
$$
P(h|D)=\frac {P(h)·P(D|h)}{P(D)} =(near)P(h)·P(D|h)
$$
​		P(D)均一样，因此是正比关系。P（h）就是先验概率。

## 估计方式

### 极大似然估计

​		最符合观测数据P（D|h）的样本比较有优势

### 奥卡姆剃刀

​		最符合先验概率P（h）较大的模型有较大优势。因为他假定能用最少量的东西完成的就不用大量的来完成。

## 例子2 垃圾邮件分类

​		D表示这封邮件，D由n个单词组组成，用h+表示垃圾邮件，h-表示正常邮件。
$$
P(h^+|D)=\frac {P(h^+)·P(D|h^+)}{P(D)}=(near)P(h^+)·P(D|h^+)
$$

$$
P(h^-|D)=\frac {P(h^-)·P(D|h^-)}{P(D)}=(near)P(h^-)·P(D|h^-)
$$

其中:
$$
P(D|h^+)=P(d_1,d_2,…,d_n|h^+)
$$
​		可拓展为**朴素贝叶斯分类器**，假定对已知类别，假设所有属性之间相互独立，即每个属性单独的对结果造成影响“
$$
P(c|x)=\frac {P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{i=1}^nP(x_i|c)
$$
因此朴素贝叶斯判定准则，用来枚举选最大：
$$
h_{nb}(x)=arg\ max\ P(c)\prod_{i=1}^dP(x_i|c)
$$