# 自然语言处理

Deep Learning for Natural Language Processing(word to vector)

## 1.自然语言处理与深度学习

拼写检查、关键字检索

文本挖掘

文本分类

机器翻译

客服系统

复杂对话系统

## 2.语言模型

$p(S)=p(w_1,w_2,...,w_n)=p(w_1)p(w_2|w_1)p(w_3|w_2,w_1)p(w_n|w_1,w_2,...,w_n)$

$p(S)$被称为语言模型，即用来计算一个句子概率的模型。每个词的出现概率和前面那些词有关系。

1. 数据过于稀疏
2. 参数空间太大

## 3.N-gram模型

1. 假设下一个词的出现依赖于它前面的一个词
2. 假设下一个词的出现依赖于它前面的两个词

I want Chinese food.

$p(I\ want\ Chinese food)=p(I)\times p(want|I)\times p(Chinese|want)\times p(food|Chinese)$

## 4.词向量

$word2vector$:相似的词语相互之间应该更近（$nearest$），所包含的关系

所构造出来的词向量并不是和词的拼写相关，而是跟==语言逻辑==相关

## 5.神经网络模型

## 6.Hierarchiral SoftMax（分层）

CBOW(Continuous Bag-of-word Model)是一种基于上下文的词语预测当前词语的出现概率的模型。

哈夫曼树是一种带权路径长度最短的二叉树，也称为最优二叉树。

将不同词频的词分层判断，将比较重要的词放在前面

Logistic回归(二分类任务)
$$
h_\theta(x)=g(\theta^Tx)=\frac {1}{1+e^{-\theta^Tx}}
$$
