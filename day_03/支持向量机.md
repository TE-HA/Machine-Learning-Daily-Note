# 支持向量机SVM

Support Vector Machine

## 1.解决问题：找出最好的决策边界

1. 能将类别分开
2. 找到”最大间隔“（maximum margin）的划分超平面，增强模型的泛化能力

支持向量：距离超平面最近的几个训练样本使得满足分类任务

## 2.距离和数据的定义

数据标签定义

二分类：

X为正例的时候Y=+1，当X为负例时候Y=-1

决策方程（预测函数）：即预测值>0的时候认为其为正例，当预测值<0的时候则认为其为负例。

优化目标：

通俗解释：找到一条直线（w和b），使得离该线最近的点的距离越远越好

将点到直线的距离化简为：
$$
\frac {y_i·（\omega^T·\Phi(x_i)+b）}{||w||}
$$
$\Phi(x_i)$表示对数据的处理

==放缩变换==：对于决策方程（w，b）可以通过方所使得其结果值|Y|>=1

优化目标：
$$
arg\ max\{\frac {1}{||w||}min[y_i·(w^T·\Phi(x_i)+b)]\}
$$
由于上述放缩变换，只需要考虑$arg\ max\frac {1}{||w||}$即可

==>当前目标：$max_{w,b}\frac {1}{||w||}$，约束条件：$y_i(w^T·\Phi(x_i)+b)\ge1$

常规套路：将求解极大值问题转换为极小值问题>==$min_{w,b}\frac {1}{2}w^2$==

拉格朗日乘子法（带约束性的优化问题）（对偶问题）：分别对w和b求偏导==0

支持向量:真正发挥作用的数据点，$\alpha$值不为0的样本

## 3.核函数

针对线性分类无法处理的问题。可将样本从原始空间映射到更高维的特征空间，使得样本在这个特征空间内线性可分。（线性核、多项式核、高斯核、拉普拉斯和、$Sigmod$核）

软间隔：允许SVM在一些样本上出错

硬间隔：所有样本都必须划分正确